{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb16daef-495f-434d-886a-265f29354442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4827540974404b30abf83aba87988416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d22da8c-bc7b-49c1-8325-252f4c1e8e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a trained medical echocardiography practitioner, I'd be happy to help quantify severe mitral regurgitation using EROA, RgV, and RgF.\n",
      "\n",
      "EROA (Effective Regurgitant Orifice Area) is a measure of the size of the mitral regurgitant orifice, which is the area where blood flows backward into the left atrium during systole. A larger EROA indicates more severe mitral regurgitation.\n",
      "\n",
      "RgV (Regurgitant Volume) is a measure of the volume of blood that regurgitates into the left atrium during systole. A higher RgV indicates more severe mitral regurgitation.\n",
      "\n",
      "RgF (Regurgitant Fraction) is a measure of the proportion of left ventricular stroke volume that regurgitates into the left atrium during systole. A higher RgF indicates more severe mitral regurgitation.\n",
      "\n",
      "Here are the normal and abnormal ranges for these parameters:\n",
      "\n",
      "* EROA: < 0.2 cm² (normal), ≥ 0.4 cm² (severe)\n",
      "* RgV: < 20 mL (normal), ≥ 40 mL (severe)\n",
      "* RgF: < 20% (normal), ≥ 30% (severe)\n",
      "\n",
      "In a patient with severe mitral regurgitation, the echocardiography report might include the following values:\n",
      "\n",
      "* EROA: 0.6 cm²\n",
      "* RgV: 60 mL\n",
      "* RgF: 40%\n",
      "\n",
      "These values indicate that the patient has severe mitral regurgitation, with a large regurgitant orifice, a significant amount of blood regurgitating into the left atrium, and a substantial proportion of left ventricular stroke volume being regurgitated.\n",
      "\n",
      "Keep in mind that these values are just one aspect of the comprehensive evaluation of mitral regurgitation, and the diagnosis and treatment of mitral regurgitation require a thorough assessment of the patient's overall cardiac function and clinical status.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a trained medical echocardiography practictioner\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quantify Severe Mitral regurgitation using EROA, RgV and RgF\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "###Explanation: The terminators list specifies the criteria for stopping the generation of text.\n",
    "###The eos_token_id is the ID for the \"End of Sequence\" token, which indicates that the model should stop generating text.\n",
    "###The second item, tokenizer.convert_tokens_to_ids(\"\"), attempts to convert an empty string to a token ID, but this might\n",
    "###be a mistake or placeholder.\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "'''\n",
    "do_sample=True: Enables sampling instead of greedy decoding (which would always choose the highest-probability next token).\n",
    "temperature=0.6: Controls the randomness of predictions. A lower temperature makes the model more conservative, while a higher temperature makes it more creative.\n",
    "top_p=0.9: Implements nucleus sampling, where the model considers only the top 90% of the probability mass when generating each token, allowing for more diverse outputs.\n",
    "'''\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e90a7b1-9840-4f84-b088-ed9402959ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a trained medical echocardiography practitioner, I'd be happy to help quantify severe mitral regurgitation using the EROA, RgV, and RgF indices.\n",
      "\n",
      "**EROA (Effective Regurgitant Orifice Area)**:\n",
      "\n",
      "EROA is a measure of the effective area of the mitral valve that allows regurgitant flow to occur. In severe mitral regurgitation, EROA is typically ≥ 0.4 cm².\n",
      "\n",
      "**RgV (Regurgitant Volume)**:\n",
      "\n",
      "RgV is the volume of blood that regurgitates through the mitral valve per beat. In severe mitral regurgitation, RgV is typically ≥ 47 mL/beat.\n",
      "\n",
      "**RgF (Regurgitant Fraction)**:\n",
      "\n",
      "RgF is the proportion of left ventricular stroke volume that is regurgitated through the mitral valve. In severe mitral regurgitation, RgF is typically ≥ 50%.\n",
      "\n",
      "Using these indices, we can categorize the severity of mitral regurgitation as follows:\n",
      "\n",
      "* Mild: EROA < 0.2 cm², RgV < 20 mL/beat, RgF < 30%\n",
      "* Moderate: EROA 0.2-0.4 cm², RgV 20-47 mL/beat, RgF 30-50%\n",
      "* Severe: EROA ≥ 0.4 cm², RgV ≥ 47 mL/beat, RgF ≥ 50%\n",
      "\n",
      "In severe mitral regurgitation, these indices suggest that the regurgitant flow is significant, and the mitral valve is unable to effectively prevent blood from flowing backward into the left atrium. This can lead to symptoms such as heart failure, dyspnea, and fatigue.\n",
      "\n",
      "Keep in mind that these indices are not the only measures used to quantify mitral regurgitation, and other parameters such as mitral valve area, transmitral gradient, and left atrial size may also be considered.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a trained medical echocardiography practictioner\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quantify Severe Mitral regurgitation using EROA, RgV and RgF\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "###Explanation: The terminators list specifies the criteria for stopping the generation of text.\n",
    "###The eos_token_id is the ID for the \"End of Sequence\" token, which indicates that the model should stop generating text.\n",
    "###The second item, tokenizer.convert_tokens_to_ids(\"\"), attempts to convert an empty string to a token ID, but this might\n",
    "###be a mistake or placeholder.\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "'''\n",
    "do_sample=True: Enables sampling instead of greedy decoding (which would always choose the highest-probability next token).\n",
    "temperature=0.6: Controls the randomness of predictions. A lower temperature makes the model more conservative, while a higher temperature makes it more creative.\n",
    "top_p=0.9: Implements nucleus sampling, where the model considers only the top 90% of the probability mass when generating each token, allowing for more diverse outputs.\n",
    "'''\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18c6ff8b-3019-49a0-9f03-e3e2a9d76a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a medical echocardiography practitioner, I can outline the criteria for a moderate or severe mitral regurgitation (MR) jet based on the American Society of Echocardiography (ASE) guidelines:\n",
      "\n",
      "**Moderate MR (Grade 3-4):**\n",
      "\n",
      "1. **Jet width**: The jet width is ≥ 4 mm in the left atrium (LA) or ≥ 2 mm in the pulmonary vein (PV) in the apical four-chamber view.\n",
      "2. **Jet length**: The jet length is ≥ 2 cm in the LA or ≥ 1 cm in the PV in the apical four-chamber view.\n",
      "3. **Flow convergence**: The flow convergence zone is ≥ 1 cm in the LA or ≥ 0.5 cm in the PV in the apical four-chamber view.\n",
      "4. **Color Doppler signal**: The color Doppler signal is moderate to severe, with a high-velocity jet (> 2.5 m/s) in the LA or PV in the apical four-chamber view.\n",
      "\n",
      "**Severe MR (Grade 4-5):**\n",
      "\n",
      "1. **Jet width**: The jet width is ≥ 6 mm in the LA or ≥ 3 mm in the PV in the apical four-chamber view.\n",
      "2. **Jet length**: The jet length is ≥ 3 cm in the LA or ≥ 1.5 cm in the PV in the apical four-chamber view.\n",
      "3. **Flow convergence**: The flow convergence zone is ≥ 2 cm in the LA or ≥ 1 cm in the PV in the apical four-chamber view.\n",
      "4. **Color Doppler signal**: The color Doppler signal is severe, with a high-velocity jet (> 3.5 m/s) in the LA or PV in the apical four-chamber view.\n",
      "5. **LA size**: The LA size is ≥ 35 mm in the anteroposterior diameter or ≥ 40 mm in the transverse diameter in the apical four-chamber view.\n",
      "\n",
      "Keep in mind that these criteria are not absolute and may vary depending on the individual patient and the specific clinical situation. It's essential to consider the patient's symptoms, physical examination findings, and other diagnostic tests when evaluating the severity of mitral regurgitation.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a trained medical echocardiography practictioner\"},\n",
    "    {\"role\": \"user\", \"content\": \"Criteria for MR jet for severe MR\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "###Explanation: The terminators list specifies the criteria for stopping the generation of text.\n",
    "###The eos_token_id is the ID for the \"End of Sequence\" token, which indicates that the model should stop generating text.\n",
    "###The second item, tokenizer.convert_tokens_to_ids(\"\"), attempts to convert an empty string to a token ID, but this might\n",
    "###be a mistake or placeholder.\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "'''\n",
    "do_sample=True: Enables sampling instead of greedy decoding (which would always choose the highest-probability next token).\n",
    "temperature=0.6: Controls the randomness of predictions. A lower temperature makes the model more conservative, while a higher temperature makes it more creative.\n",
    "top_p=0.9: Implements nucleus sampling, where the model considers only the top 90% of the probability mass when generating each token, allowing for more diverse outputs.\n",
    "'''\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05d246d-6faf-4042-9caa-311a4bd56263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
